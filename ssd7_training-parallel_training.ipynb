{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD7 Training Tutorial\n",
    "\n",
    "This tutorial explains how to train an SSD7 on the Udacity road traffic datasets, and just generally how to use this SSD implementation.\n",
    "\n",
    "Disclaimer about SSD7:\n",
    "As you will see below, training SSD7 on the aforementioned datasets yields alright results, but I'd like to emphasize that SSD7 is not a carefully optimized network architecture. The idea was just to build a low-complexity network that is fast (roughly 127 FPS or more than 3 times as fast as SSD300 on a GTX 1070) for testing purposes. Would slightly different anchor box scaling factors or a slightly different number of filters in individual convolution layers make SSD7 significantly better at similar complexity? I don't know, I haven't tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "#from models.keras_ssd7_quantize import build_model_quantize\n",
    "from models.keras_ssd7_quantize2 import build_model_quantize2\n",
    "#from keras_loss_function.keras_ssd_loss import SSDLoss  #commented to test TF2.0\n",
    "from keras_loss_function.keras_ssd_loss_tf2 import SSDLoss # added for TF2.0\n",
    "\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_AnchorBoxes_1 import DefaultDenseQuantizeConfig\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "## imports used for pruning\n",
    "import tensorflow_model_optimization as tfmot \n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "## loading tensorboard\n",
    "\n",
    "#%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Device found : {physical_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set the model configuration parameters\n",
    "\n",
    "The cell below sets a number of parameters that define the model configuration. The parameters set here are being used both by the `build_model()` function that builds the model as well as further down by the constructor for the `SSDInputEncoder` object that is needed to to match ground truth and anchor boxes during the training.\n",
    "\n",
    "Here are just some comments on a few of the parameters, read the documentation for more details:\n",
    "\n",
    "* Set the height, width, and number of color channels to whatever you want the model to accept as image input. If your input images have a different size than you define as the model input here, or if your images have non-uniform size, then you must use the data generator's image transformations (resizing and/or cropping) so that your images end up having the required input size before they are fed to the model. to convert your images to the model input size during training. The SSD300 training tutorial uses the same image pre-processing and data augmentation as the original Caffe implementation, so take a look at that to see one possibility of how to deal with non-uniform-size images.\n",
    "* The number of classes is the number of positive classes in your dataset, e.g. 20 for Pascal VOC or 80 for MS COCO. Class ID 0 must always be reserved for the background class, i.e. your positive classes must have positive integers as their IDs in your dataset.\n",
    "* The `mode` argument in the `build_model()` function determines whether the model will be built with or without a `DecodeDetections` layer as its last layer. In 'training' mode, the model outputs the raw prediction tensor, while in 'inference' and 'inference_fast' modes, the raw predictions are being decoded into absolute coordinates and filtered via confidence thresholding, non-maximum suppression, and top-k filtering. The difference between latter two modes is that 'inference' uses the decoding procedure of the original Caffe implementation, while 'inference_fast' uses a faster, but possibly less accurate decoding procedure.\n",
    "* The reason why the list of scaling factors has 5 elements even though there are only 4 predictor layers in tSSD7 is that the last scaling factor is used for the second aspect-ratio-1 box of the last predictor layer. Refer to the documentation for details.\n",
    "* `build_model()` and `SSDInputEncoder` have two arguments for the anchor box aspect ratios: `aspect_ratios_global` and `aspect_ratios_per_layer`. You can use either of the two, you don't need to set both. If you use `aspect_ratios_global`, then you pass one list of aspect ratios and these aspect ratios will be used for all predictor layers. Every aspect ratio you want to include must be listed once and only once. If you use `aspect_ratios_per_layer`, then you pass a nested list containing lists of aspect ratios for each individual predictor layer. This is what the SSD300 training tutorial does. It's your design choice whether all predictor layers should use the same aspect ratios or whether you think that for your dataset, certain aspect ratios are only necessary for some predictor layers but not for others. Of course more aspect ratios means more predicted boxes, which in turn means increased computational complexity.\n",
    "* If `two_boxes_for_ar1 == True`, then each predictor layer will predict two boxes with aspect ratio one, one a bit smaller, the other one a bit larger.\n",
    "* If `clip_boxes == True`, then the anchor boxes will be clipped so that they lie entirely within the image boundaries. It is recommended not to clip the boxes. The anchor boxes form the reference frame for the localization prediction. This reference frame should be the same at every spatial position.\n",
    "* In the matching process during the training, the anchor box offsets are being divided by the variances. Leaving them at 1.0 for each of the four box coordinates means that they have no effect. Setting them to less than 1.0 spreads the imagined anchor box offset distribution for the respective box coordinate.\n",
    "* `normalize_coords` converts all coordinates from absolute coordinate to coordinates that are relative to the image height and width. This setting has no effect on the outcome of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the input images\n",
    "img_width = 480 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "intensity_mean = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "intensity_range = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "n_classes = 5 # Number of positive classes\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "aspect_ratios = [0.5, 1.0, 2.0] # The list of aspect ratios for the anchor boxes\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build or load the model\n",
    "\n",
    "You will want to execute either of the two code cells in the subsequent two sub-sections, not both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a new model\n",
    "\n",
    "If you want to create a new model, this is the relevant section for you. If you want to load a previously saved model, skip ahead to section 2.2.\n",
    "\n",
    "The code cell below does the following things:\n",
    "1. It calls the function `build_model()` to build the model.\n",
    "2. It optionally loads some weights into the model.\n",
    "3. It then compiles the model for the training. In order to do so, we're defining an optimizer (Adam) and a loss function (SSDLoss) to be passed to the `compile()` method.\n",
    "\n",
    "`SSDLoss` is a custom Keras loss function that implements the multi-task log loss for classification and smooth L1 loss for localization. `neg_pos_ratio` and `alpha` are set as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "# 2: Optional: Load some weights\n",
    "\n",
    "#model.load_weights('./ssd7_weights.h5', by_name=True)\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load a saved model\n",
    "\n",
    "If you have previously created and saved a model and would now like to load it, simply execute the next code cell. The only thing you need to do is to set the path to the saved model HDF5 file that you would like to load.\n",
    "\n",
    "The SSD model contains custom objects: Neither the loss function, nor the anchor box or detection decoding layer types are contained in the Keras core library, so we need to provide them to the model loader.\n",
    "\n",
    "This next code cell assumes that you want to load a model that was created in 'training' mode. If you want to load a model that was created in 'inference' or 'inference_fast' mode, you'll have to add the `DecodeDetections` or `DecodeDetectionsFast` layer type to the `custom_objects` dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/anaconda2/envs/mltf115_1/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mohan/anaconda2/envs/mltf115_1/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mohan/anaconda2/envs/mltf115_1/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mohan/anaconda2/envs/mltf115_1/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "model_path = './striped_pruned_polynomial_20to80p_from_base_model_2202.h5'\n",
    "#model_path = './saved_models/base_model_13_01/trained_a_base_model_1301.h5'\n",
    "\n",
    "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "#K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                               'compute_loss': ssd_loss.compute_loss})\n",
    "\n",
    "#model = load_model(model_path, custom_objects={'compute_loss': ssd_loss.compute_loss})\n",
    "\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 480, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 480, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 480, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_stddev_normalization (Lam (None, 300, 480, 3)  0           input_mean_normalization[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 300, 480, 32) 2432        input_stddev_normalization[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 300, 480, 32) 128         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu1 (ELU)                      (None, 300, 480, 32) 0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 240, 32) 0           elu1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 150, 240, 48) 13872       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2 (BatchNormalization)        (None, 150, 240, 48) 192         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu2 (ELU)                      (None, 150, 240, 48) 0           bn2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 120, 48)  0           elu2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 75, 120, 64)  27712       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)        (None, 75, 120, 64)  256         conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu3 (ELU)                      (None, 75, 120, 64)  0           bn3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 37, 60, 64)   0           elu3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 37, 60, 64)   36928       pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn4 (BatchNormalization)        (None, 37, 60, 64)   256         conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu4 (ELU)                      (None, 37, 60, 64)   0           bn4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 18, 30, 64)   0           elu4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 18, 30, 48)   27696       pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn5 (BatchNormalization)        (None, 18, 30, 48)   192         conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu5 (ELU)                      (None, 18, 30, 48)   0           bn5[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 9, 15, 48)    0           elu5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 9, 15, 48)    20784       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn6 (BatchNormalization)        (None, 9, 15, 48)    192         conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu6 (ELU)                      (None, 9, 15, 48)    0           bn6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool6 (MaxPooling2D)            (None, 4, 7, 48)     0           elu6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 4, 7, 32)     13856       pool6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn7 (BatchNormalization)        (None, 4, 7, 32)     128         conv7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu7 (ELU)                      (None, 4, 7, 32)     0           bn7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "classes4 (Conv2D)               (None, 37, 60, 24)   13848       elu4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes5 (Conv2D)               (None, 18, 30, 24)   10392       elu5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes6 (Conv2D)               (None, 9, 15, 24)    10392       elu6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes7 (Conv2D)               (None, 4, 7, 24)     6936        elu7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes4 (Conv2D)                 (None, 37, 60, 16)   9232        elu4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes5 (Conv2D)                 (None, 18, 30, 16)   6928        elu5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes6 (Conv2D)                 (None, 9, 15, 16)    6928        elu6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes7 (Conv2D)                 (None, 4, 7, 16)     4624        elu7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes4_reshape (Reshape)      (None, None, 6)      0           classes4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes5_reshape (Reshape)      (None, None, 6)      0           classes5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes6_reshape (Reshape)      (None, None, 6)      0           classes6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes7_reshape (Reshape)      (None, None, 6)      0           classes7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors4 (AnchorBoxes)          (None, 37, 60, 4, 8) 0           boxes4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors5 (AnchorBoxes)          (None, 18, 30, 4, 8) 0           boxes5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors6 (AnchorBoxes)          (None, 9, 15, 4, 8)  0           boxes6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors7 (AnchorBoxes)          (None, 4, 7, 4, 8)   0           boxes7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "classes_concat (Concatenate)    (None, None, 6)      0           classes4_reshape[0][0]           \n",
      "                                                                 classes5_reshape[0][0]           \n",
      "                                                                 classes6_reshape[0][0]           \n",
      "                                                                 classes7_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "boxes4_reshape (Reshape)        (None, None, 4)      0           boxes4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "boxes5_reshape (Reshape)        (None, None, 4)      0           boxes5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "boxes6_reshape (Reshape)        (None, None, 4)      0           boxes6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "boxes7_reshape (Reshape)        (None, None, 4)      0           boxes7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors4_reshape (Reshape)      (None, None, 8)      0           anchors4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors5_reshape (Reshape)      (None, None, 8)      0           anchors5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors6_reshape (Reshape)      (None, None, 8)      0           anchors6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors7_reshape (Reshape)      (None, None, 8)      0           anchors7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes_softmax (Activation)    (None, None, 6)      0           classes_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "boxes_concat (Concatenate)      (None, None, 4)      0           boxes4_reshape[0][0]             \n",
      "                                                                 boxes5_reshape[0][0]             \n",
      "                                                                 boxes6_reshape[0][0]             \n",
      "                                                                 boxes7_reshape[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "anchors_concat (Concatenate)    (None, None, 8)      0           anchors4_reshape[0][0]           \n",
      "                                                                 anchors5_reshape[0][0]           \n",
      "                                                                 anchors6_reshape[0][0]           \n",
      "                                                                 anchors7_reshape[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 18)     0           classes_softmax[0][0]            \n",
      "                                                                 boxes_concat[0][0]               \n",
      "                                                                 anchors_concat[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 213,904\n",
      "Trainable params: 213,232\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_finroc_model/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.1 Compile if you are retraining a pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load a quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "\n",
    "with quantize_scope():\n",
    "    #quantized_model = tf.keras.models.load_model('./saved_models/quantized_model_27_01/ssd7_quant_epoch-04_loss-2.5155_val_loss-2.5268.h5', custom_objects={'AnchorBoxes': AnchorBoxes, 'compute_loss': ssd_loss.compute_loss})\n",
    "    quant_aware_model = tf.keras.models.load_model('./saved_models/pruned_quantized_models/ssd7_pruned_50p_quantized_1802_epoch-29_loss-1.9227_val_loss-2.1054.h5', custom_objects={'AnchorBoxes': AnchorBoxes, 'compute_loss': ssd_loss.compute_loss})\n",
    "    #loaded_model = tf.keras.models.load_model('./quantize_ready_model_20_01_Conv2D.h5')\n",
    "\n",
    "#with quantize_scope(\n",
    "#          {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "#           'AnchorBoxes': AnchorBoxes}):    \n",
    "#    loaded_model = tf.keras.models.load_model('./ssd7_quant_epoch-04_loss-2.5155_val_loss-2.5268.h5', custom_objects={'AnchorBoxes': AnchorBoxes})\n",
    "    \n",
    "#loaded_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "#quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "quant_aware_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "with tfmot.sparsity.keras.prune_scope():\n",
    "  pruned_model = tf.keras.models.load_model('./saved_models/pruned_models/50percent/1502/striped_pruned_model_1501_50p.h5', custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                               'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up the data generators for the training\n",
    "\n",
    "The code cells below set up data generators for the training and validation datasets to train the model. You will have to set the file paths to your dataset. Depending on the annotations format of your dataset, you might also have to switch from the CSV parser to the XML or JSON parser, or you might have to write a new parser method in the `DataGenerator` class that can handle whatever format your annotations are in. The [README](https://github.com/pierluigiferrari/ssd_keras/blob/master/README.md) of this repository provides a summary of the design of the `DataGenerator`, which should help you in case you need to write a new parser or adapt one of the existing parsers to your needs.\n",
    "\n",
    "Note that the generator provides two options to speed up the training. By default, it loads the individual images for a batch from disk. This has two disadvantages. First, for compressed image formats like JPG, this is a huge computational waste, because every image needs to be decompressed again and again every time it is being loaded. Second, the images on disk are likely not stored in a contiguous block of memory, which may also slow down the loading process. The first option that `DataGenerator` provides to deal with this is to load the entire dataset into memory, which reduces the access time for any image to a negligible amount, but of course this is only an option if you have enough free memory to hold the whole dataset. As a second option, `DataGenerator` provides the possibility to convert the dataset into a single HDF5 file. This HDF5 file stores the images as uncompressed arrays in a contiguous block of memory, which dramatically speeds up the loading time. It's not as good as having the images in memory, but it's a lot better than the default option of loading them from their compressed JPG state every time they are needed. Of course such an HDF5 dataset may require significantly more disk space than the compressed images. You can later load these HDF5 datasets directly in the constructor.\n",
    "\n",
    "Set the batch size to to your preference and to what your GPU memory allows, it's not the most important hyperparameter. The Caffe implementation uses a batch size of 32, but smaller batch sizes work fine, too.\n",
    "\n",
    "The `DataGenerator` itself is fairly generic. I doesn't contain any data augmentation or bounding box encoding logic. Instead, you pass a list of image transformations and an encoder for the bounding boxes in the `transformations` and `label_encoder` arguments of the data generator's `generate()` method, and the data generator will then apply those given transformations and the encoding to the data. Everything here is preset already, but if you'd like to learn more about the data generator and its data augmentation capabilities, take a look at the detailed tutorial in [this](https://github.com/pierluigiferrari/data_generator_object_detection_2d) repository.\n",
    "\n",
    "The image processing chain defined further down in the object named `data_augmentation_chain` is just one possibility of what a data augmentation pipeline for unform-size images could look like. Feel free to put together other image processing chains, you can use the `DataAugmentationConstantInputSize` class as a template. Or you could use the original SSD data augmentation pipeline by instantiting an `SSDDataAugmentation` object and passing that to the generator instead. This procedure is not exactly efficient, but it evidently produces good results on multiple datasets.\n",
    "\n",
    "An `SSDInputEncoder` object, `ssd_input_encoder`, is passed to both the training and validation generators. As explained above, it matches the ground truth labels to the model's anchor boxes and encodes the box coordinates into the format that the model needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "The example setup below was used to train SSD7 on two road traffic datasets released by [Udacity](https://github.com/udacity/self-driving-car/tree/master/annotations) with around 20,000 images in total and 5 object classes (car, truck, pedestrian, bicyclist, traffic light), although the vast majority of the objects are cars. The original datasets have a constant image size of 1200x1920 RGB. I consolidated the two datasets, removed a few bad samples (although there are probably many more), and resized the images to 300x480 RGB, i.e. to one sixteenth of the original image size. In case you'd like to train a model on the same dataset, you can download the consolidated and resized dataset I used [here](https://drive.google.com/open?id=1tfBFavijh4UTG4cGqIKwhcklLXUDuY0D) (about 900 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t 18000\n",
      "Number of images in the validation dataset:\t  4241\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# TODO: Set the paths to your dataset here.\n",
    "\n",
    "# Images\n",
    "images_dir = '../ssd_pruning/udacity_driving_datasets/'\n",
    "\n",
    "# Ground truth\n",
    "train_labels_filename = '../ssd_pruning/udacity_driving_datasets/labels_train_car.csv'\n",
    "val_labels_filename   = '../ssd_pruning/udacity_driving_datasets/labels_val_car.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "# want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "\n",
    "#train_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_train.h5',\n",
    "#                                  resize=False,\n",
    "#                                  variable_image_size=True,\n",
    "#                                  verbose=True)\n",
    "#\n",
    "#val_dataset.create_hdf5_dataset(file_path='dataset_udacity_traffic_val.h5',\n",
    "#                                resize=False,\n",
    "#                                variable_image_size=True,\n",
    "#                                verbose=True)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doned\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# 4: Define the image processing chain.\n",
    "\n",
    "data_augmentation_chain = DataAugmentationConstantInputSize(random_brightness=(-48, 48, 0.5),\n",
    "                                                            random_contrast=(0.5, 1.8, 0.5),\n",
    "                                                            random_saturation=(0.5, 1.8, 0.5),\n",
    "                                                            random_hue=(18, 0.5),\n",
    "                                                            random_flip=0.5,\n",
    "                                                            random_translate=((0.03,0.5), (0.03,0.5), 0.5),\n",
    "                                                            random_scale=(0.5, 2.0, 0.5),\n",
    "                                                            n_trials_max=3,\n",
    "                                                            clip_boxes=True,\n",
    "                                                            overlap_criterion='area',\n",
    "                                                            bounds_box_filter=(0.3, 1.0),\n",
    "                                                            bounds_validator=(0.5, 1.0),\n",
    "                                                            n_boxes_min=1,\n",
    "                                                            background=(0,0,0))\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "\n",
    "'''\n",
    "predictor_sizes = [quant_aware_model.get_layer('quant_classes4').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('quant_classes5').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('quant_classes6').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('quant_classes7').output_shape[1:3]]\n",
    "'''\n",
    "'''\n",
    "predictor_sizes = [quant_aware_model.get_layer('classes4').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('classes5').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('classes6').output_shape[1:3],\n",
    "                   quant_aware_model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "'''\n",
    "#'''\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "#'''\n",
    "\n",
    "'''\n",
    "predictor_sizes = [pruned_model.get_layer('classes4').output_shape[1:3],\n",
    "                   pruned_model.get_layer('classes5').output_shape[1:3],\n",
    "                   pruned_model.get_layer('classes6').output_shape[1:3],\n",
    "                   pruned_model.get_layer('classes7').output_shape[1:3]]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[data_augmentation_chain],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "print('Doned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Value', train_dataset_size/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set the remaining training parameters and train the unpruned-model\n",
    "\n",
    "We've already chosen an optimizer and a learning rate and set the batch size above, now let's set the remaining training parameters.\n",
    "\n",
    "I'll set a few Keras callbacks below, one for early stopping, one to reduce the learning rate if the training stagnates, one to save the best models during the training, and one to continuously stream the training history to a CSV file after every epoch. Logging to a CSV file makes sense, because if we didn't do that, in case the training terminates with an exception at some point or if the kernel of this Jupyter notebook dies for some reason or anything like that happens, we would lose the entire history for the trained epochs. Feel free to add more callbacks if you want TensorBoard summaries or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the weights.\n",
    "model_checkpoint = ModelCheckpoint(filepath='ssd7_polynomial_20pto80p_quantized_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd7_training_log_polynomial_20pto80p_quantized.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "#logdir = 'logs_tboard'\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "callbacks_unpruned = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll set one epoch to consist of 1,000 training steps I'll arbitrarily set the number of epochs to 20 here. This does not imply that 20,000 training steps is the right number. Depending on the model, the dataset, the learning rate, etc. you might have to train much longer to achieve convergence, or maybe less.\n",
    "\n",
    "Instead of trying to train a model to convergence in one go, you might want to train only for a few epochs at a time.\n",
    "\n",
    "In order to only run a partial training and resume smoothly later on, there are a few things you should note:\n",
    "1. Always load the full model if you can, rather than building a new model and loading previously saved weights into it. Optimizers like SGD or Adam keep running averages of past gradient moments internally. If you always save and load full models when resuming a training, then the state of the optimizer is maintained and the training picks up exactly where it left off. If you build a new model and load weights into it, the optimizer is being initialized from scratch, which, especially in the case of Adam, leads to small but unnecessary setbacks every time you resume the training with previously saved weights.\n",
    "2. You should tell `fit_generator()` which epoch to start from, otherwise it will start with epoch 0 every time you resume the training. Set `initial_epoch` to be the next epoch of your training. Note that this parameter is zero-based, i.e. the first epoch is epoch 0. If you had trained for 10 epochs previously and now you'd want to resume the training from there, you'd set `initial_epoch = 10` (since epoch 10 is the eleventh epoch). Furthermore, set `final_epoch` to the last epoch you want to run. To stick with the previous example, if you had trained for 10 epochs previously and now you'd want to train for another 10 epochs, you'd set `initial_epoch = 10` and `final_epoch = 20`.\n",
    "3. Callbacks like `ModelCheckpoint` or `ReduceLROnPlateau` are stateful, so you might want ot save their state somehow if you want to pick up a training exactly where you left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train unpruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs_tboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Set the epochs to train for.\n",
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 30\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks_unpruned,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.save(model, './saved_finroc_model/')\n",
    "print('Inputs --> ', model.inputs)\n",
    "print('Outputs --> ', model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model_for_pruning.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to ./model/tf_model.pb\n",
    "tf.train.write_graph(frozen_graph, \"model\", \"tf_model_20p_to_80p_polynomial_base_pruned_2202.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the training and validation loss evolved to check whether our training is going in the right direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Saving unpruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_a_base_model_30ep_1502.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/mlp/git/ssd_keras/keras_loss_function/keras_ssd_loss_tf2.py:76: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "quant_aware_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1970Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 43s - loss: 2.5720\n",
      "Epoch 00001: val_loss improved from inf to 2.57204, saving model to ssd7_polynomial_20pto80p_quantized_epoch-01_loss-2.1968_val_loss-2.5720.h5\n",
      "1000/1000 [==============================] - 246s 246ms/step - loss: 2.1968 - val_loss: 2.5720\n",
      "Epoch 2/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1763Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.3620\n",
      "Epoch 00002: val_loss improved from 2.57204 to 2.36203, saving model to ssd7_polynomial_20pto80p_quantized_epoch-02_loss-2.1758_val_loss-2.3620.h5\n",
      "1000/1000 [==============================] - 227s 227ms/step - loss: 2.1758 - val_loss: 2.3620\n",
      "Epoch 3/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1624Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.5316\n",
      "Epoch 00003: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1620 - val_loss: 2.5316\n",
      "Epoch 4/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1674Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.5257\n",
      "Epoch 00004: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1672 - val_loss: 2.5257\n",
      "Epoch 5/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1606Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.6188\n",
      "Epoch 00005: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1606 - val_loss: 2.6188\n",
      "Epoch 6/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1593Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.3986\n",
      "Epoch 00006: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1596 - val_loss: 2.3986\n",
      "Epoch 7/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1702Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.4144\n",
      "Epoch 00007: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1694 - val_loss: 2.4144\n",
      "Epoch 8/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1611Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.4241\n",
      "Epoch 00008: val_loss did not improve from 2.36203\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1613 - val_loss: 2.4241\n",
      "Epoch 9/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1710Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.3583\n",
      "Epoch 00009: val_loss improved from 2.36203 to 2.35828, saving model to ssd7_polynomial_20pto80p_quantized_epoch-09_loss-2.1709_val_loss-2.3583.h5\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1709 - val_loss: 2.3583\n",
      "Epoch 10/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1793Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.3509\n",
      "Epoch 00010: val_loss improved from 2.35828 to 2.35090, saving model to ssd7_polynomial_20pto80p_quantized_epoch-10_loss-2.1789_val_loss-2.3509.h5\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1789 - val_loss: 2.3509\n",
      "Epoch 11/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1671Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.3540\n",
      "Epoch 00011: val_loss did not improve from 2.35090\n",
      "1000/1000 [==============================] - 228s 228ms/step - loss: 2.1669 - val_loss: 2.3540\n",
      "Epoch 12/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1773Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.4583\n",
      "Epoch 00012: val_loss did not improve from 2.35090\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 2.1776 - val_loss: 2.4583\n",
      "Epoch 13/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1534Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.3478\n",
      "Epoch 00013: val_loss improved from 2.35090 to 2.34785, saving model to ssd7_polynomial_20pto80p_quantized_epoch-13_loss-2.1533_val_loss-2.3478.h5\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1533 - val_loss: 2.3478\n",
      "Epoch 14/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1743Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.7382\n",
      "Epoch 00014: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 228s 228ms/step - loss: 2.1742 - val_loss: 2.7382\n",
      "Epoch 15/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1741Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.5864\n",
      "Epoch 00015: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1739 - val_loss: 2.5864\n",
      "Epoch 16/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1731Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.4839\n",
      "Epoch 00016: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1727 - val_loss: 2.4839\n",
      "Epoch 17/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1688Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.7611\n",
      "Epoch 00017: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 231s 231ms/step - loss: 2.1686 - val_loss: 2.7611\n",
      "Epoch 18/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1496Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.5321\n",
      "Epoch 00018: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1496 - val_loss: 2.5321\n",
      "Epoch 19/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1511Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.4713\n",
      "Epoch 00019: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 232s 232ms/step - loss: 2.1511 - val_loss: 2.4713\n",
      "Epoch 20/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1554Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.4410\n",
      "Epoch 00020: val_loss did not improve from 2.34785\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.1560 - val_loss: 2.4410\n",
      "Epoch 21/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.1517Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.3860\n",
      "Epoch 00021: val_loss did not improve from 2.34785\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "1000/1000 [==============================] - 232s 232ms/step - loss: 2.1526 - val_loss: 2.3860\n",
      "Epoch 22/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.0313Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.2533\n",
      "Epoch 00022: val_loss improved from 2.34785 to 2.25325, saving model to ssd7_polynomial_20pto80p_quantized_epoch-22_loss-2.0314_val_loss-2.2533.h5\n",
      "1000/1000 [==============================] - 233s 233ms/step - loss: 2.0314 - val_loss: 2.2533\n",
      "Epoch 23/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9699Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.2014\n",
      "Epoch 00023: val_loss improved from 2.25325 to 2.20137, saving model to ssd7_polynomial_20pto80p_quantized_epoch-23_loss-1.9699_val_loss-2.2014.h5\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 1.9699 - val_loss: 2.2014\n",
      "Epoch 24/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9846Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.1706\n",
      "Epoch 00024: val_loss improved from 2.20137 to 2.17055, saving model to ssd7_polynomial_20pto80p_quantized_epoch-24_loss-1.9841_val_loss-2.1706.h5\n",
      "1000/1000 [==============================] - 231s 231ms/step - loss: 1.9841 - val_loss: 2.1706\n",
      "Epoch 25/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9722Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.1446\n",
      "Epoch 00025: val_loss improved from 2.17055 to 2.14459, saving model to ssd7_polynomial_20pto80p_quantized_epoch-25_loss-1.9719_val_loss-2.1446.h5\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 1.9719 - val_loss: 2.1446\n",
      "Epoch 26/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9507Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.2291\n",
      "Epoch 00026: val_loss did not improve from 2.14459\n",
      "1000/1000 [==============================] - 228s 228ms/step - loss: 1.9507 - val_loss: 2.2291\n",
      "Epoch 27/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9432Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.1202\n",
      "Epoch 00027: val_loss improved from 2.14459 to 2.12023, saving model to ssd7_polynomial_20pto80p_quantized_epoch-27_loss-1.9438_val_loss-2.1202.h5\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 1.9438 - val_loss: 2.1202\n",
      "Epoch 28/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9380Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 42s - loss: 2.1343\n",
      "Epoch 00028: val_loss did not improve from 2.12023\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 1.9386 - val_loss: 2.1343\n",
      "Epoch 29/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9324Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.1254\n",
      "Epoch 00029: val_loss did not improve from 2.12023\n",
      "1000/1000 [==============================] - 228s 228ms/step - loss: 1.9329 - val_loss: 2.1254\n",
      "Epoch 30/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 1.9212Epoch 1/30\n",
      " 531/1000 [==============>...............] - ETA: 41s - loss: 2.1373\n",
      "Epoch 00030: val_loss did not improve from 2.12023\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 1.9210 - val_loss: 2.1373\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set the epochs to train for.\n",
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 30\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history_quant = quant_aware_model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks_unpruned,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the training and validation loss evolved to check whether our training is going in the right direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_quant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-94b9be077271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_quant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_quant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_quant' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(history_quant.history['loss'], label='loss')\n",
    "plt.plot(history_quant.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.save(model, './saved_finroc_model/')\n",
    "print('Inputs --> ', quant_aware_model.inputs)\n",
    "print('Outputs --> ', quant_aware_model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Saving quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('quantized_model_from_trained_base_model_1601.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss has been decreasing at a similar pace as the training loss, indicating that our model has been learning effectively over the last 30 epochs. We could try to train longer and see if the validation loss can be decreased further. Once the validation loss stops decreasing for a couple of epochs in a row, that's when we will want to stop training. Our final weights will then be the weights of the epoch that had the lowest validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Introducing Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# Helper function uses `prune_low_magnitude` to make only the \n",
    "# Dense layers train with pruning.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule as pruning_sched\n",
    "\n",
    "#end_step = 30000\n",
    "\n",
    "'''\n",
    "def apply_pruning_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer,pruning_schedule=pruning_sched.ConstantSparsity(0.5, 0))\n",
    "  return layer\n",
    "'''\n",
    "#'''\n",
    "def apply_pruning_to_dense2(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer,pruning_schedule=pruning_sched.PolynomialDecay(initial_sparsity=0.2,\n",
    "        final_sparsity=0.8, begin_step=0, end_step=30000))\n",
    "  return layer\n",
    "#'''\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
    "# to the layers of the model.\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_pruning_to_dense2,\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "# Defining pruning parameters\n",
    "\n",
    "#pruning_params = {\n",
    "#      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "#                                                               final_sparsity=0.80,\n",
    "#                                                               begin_step=0,\n",
    "#                                                               end_step=1000)\n",
    "#}\n",
    "\n",
    "'''\n",
    "\n",
    "####model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model_for_pruning, **pruning_params)\n",
    "\n",
    "model_for_pruning.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "#model_for_pruning.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model_for_pruning.summary()\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# Helper function uses `prune_low_magnitude` to make only the \n",
    "# Dense layers train with pruning.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule as pruning_sched\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 8\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = 40000\n",
    "end_step = 800\n",
    "\n",
    "\n",
    "#'''\n",
    "#Defining pruning parameters\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=1000)\n",
    "}\n",
    "\n",
    "#'''\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "model_for_pruning.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "#model_for_pruning.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model_for_pruning.summary()\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set the remaining training parameters and train the pruned-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the weights.\n",
    "\n",
    "import tensorflow as tf\n",
    "'''\n",
    "'''\n",
    "#model_checkpoint = ModelCheckpoint(filepath='ssd7_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "#                                   monitor='val_loss',\n",
    "#                                   verbose=1,\n",
    "#                                   save_best_only=True,\n",
    "#                                   save_weights_only=False,\n",
    "#                                   mode='auto',\n",
    "#                                   period=1)\n",
    "#'''\n",
    "#'''\n",
    "csv_logger = CSVLogger(filename='ssd7_training_base_polynomialdecay_pruned_20p_80p_2202.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "#'''                                         \n",
    "#'''\n",
    "#callbacks = [model_checkpoint,\n",
    "#             csv_logger,\n",
    "#             early_stopping,\n",
    "#             reduce_learning_rate]\n",
    "#'''\n",
    "\n",
    "\n",
    "#logdir = tempfile.mkdtemp()\n",
    "#'''\n",
    "logdir='pruning_summaries/'\n",
    "\n",
    "filepath = 'ssd7_base_polynomialdecay_pruned_20p_80p_2202.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "callbacks_pruning = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch = 100000000, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch'),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir, update_freq='epoch'),\n",
    "  checkpoint,\n",
    "  csv_logger,\n",
    "  early_stopping,\n",
    "  reduce_learning_rate\n",
    "]\n",
    "print(\"Done\")\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 30\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "print(model_for_pruning.optimizer)\n",
    "\n",
    "history2 = model_for_pruning.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=final_epoch,                      \n",
    "                    validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                    callbacks=callbacks_pruning,\n",
    "                    initial_epoch=initial_epoch)\n",
    "#'''\n",
    "#use_multiprocessing=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.save(model, './saved_finroc_model/')\n",
    "print('Inputs --> ', model_for_pruning.inputs)\n",
    "print('Outputs --> ', model_for_pruning.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Save pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.save('pruned_model_1802.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Save stripped pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_export.save('striped_pruned_polynomial_20to80p_from_base_model_2202.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Plotting pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(history2.history['loss'], label='loss')\n",
    "plt.plot(history2.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make predictions\n",
    "\n",
    "Now let's make some predictions on the validation dataset with the trained model. For convenience we'll use the validation generator which we've already set up above. Feel free to change the batch size.\n",
    "\n",
    "You can set the `shuffle` option to `False` if you would like to check the model's progress on the same image(s) over the course of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the predictions.\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=10,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate samples\n",
    "\n",
    "batch_images, batch_labels, batch_filenames = next(predict_generator)\n",
    "\n",
    "i = 9 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(batch_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Make a prediction\n",
    "\n",
    "y_pred = model_for_pruning.predict(batch_images)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's decode the raw predictions in `y_pred`.\n",
    "\n",
    "Had we created the model in 'inference' or 'inference_fast' mode, then the model's final layer would be a `DecodeDetections` layer and `y_pred` would already contain the decoded predictions, but since we created the model in 'training' mode, the model outputs raw predictions that still need to be decoded and filtered. This is what the `decode_detections()` function is for. It does exactly what the `DecodeDetections` layer would do, but using Numpy instead of TensorFlow (i.e. on the CPU instead of the GPU).\n",
    "\n",
    "`decode_detections()` with default argument values follows the procedure of the original SSD implementation: First, a very low confidence threshold of 0.01 is applied to filter out the majority of the predicted boxes, then greedy non-maximum suppression is performed per class with an intersection-over-union threshold of 0.45, and out of what is left after that, the top 200 highest confidence boxes are returned. Those settings are for precision-recall scoring purposes though. In order to get some usable final predictions, we'll set the confidence threshold much higher, e.g. to 0.5, since we're only interested in the very confident predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw prediction `y_pred`\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.5,\n",
    "                                   iou_threshold=0.45,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's draw the predicted boxes onto the image. Each predicted box says its confidence next to the category name. The ground truth boxes are also drawn onto the image in green for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist() # Set the colors for the bounding boxes\n",
    "classes = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "# Draw the ground truth boxes in green (omit the label for more clarity)\n",
    "for box in batch_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    #current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "# Draw the predicted boxes in blue\n",
    "for box in y_pred_decoded[i]:\n",
    "    xmin = box[-4]\n",
    "    ymin = box[-3]\n",
    "    xmax = box[-2]\n",
    "    ymax = box[-1]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9.1 Multiple Predictions using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# 1: Set the generator for the predictions.\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=300,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         keep_images_without_gt=False)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#'''\n",
    "\n",
    "for x in range(100):\n",
    "  # 2: Generate samples\n",
    "  batch_images, batch_labels, batch_filenames = next(predict_generator)\n",
    "  #i = 0 # Which batch item to look at\n",
    "  #print(\"Image:\", batch_filenames[i])\n",
    "  #print()\n",
    "  #print(\"Ground truth boxes:\\n\")\n",
    "  #print(batch_labels[i])\n",
    "  y_pred = quant_aware_model.predict(batch_images)\n",
    "  #print(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#'''\n",
    "\n",
    "for x in range(5):\n",
    "  # 2: Generate samples\n",
    "  batch_images, batch_labels, batch_filenames = next(predict_generator)\n",
    "  i = 0 # Which batch item to look at\n",
    "  print(\"Image:\", batch_filenames[i])\n",
    "  print()\n",
    "  print(\"Ground truth boxes:\\n\")\n",
    "  print(batch_labels[i])\n",
    "  y_pred = quant_aware_model.predict(batch_images)\n",
    "  # 4: Decode the raw prediction `y_pred`\n",
    "\n",
    "  y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.5,\n",
    "                                   iou_threshold=0.45,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "  np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "  print(\"Predicted boxes:\\n\")\n",
    "  print('   class   conf xmin   ymin   xmax   ymax')\n",
    "  print(y_pred_decoded[i])\n",
    "  # 5: Draw the predicted boxes onto the image\n",
    "\n",
    "  plt.figure(figsize=(20,12))\n",
    "  plt.imshow(batch_images[i])\n",
    "\n",
    "  current_axis = plt.gca()\n",
    "\n",
    "  colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist() # Set the colors for the bounding boxes\n",
    "  classes = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "  # Draw the ground truth boxes in green (omit the label for more clarity)\n",
    "  for box in batch_labels[i]:\n",
    "      xmin = box[1]\n",
    "      ymin = box[2]\n",
    "      xmax = box[3]\n",
    "      ymax = box[4]\n",
    "      label = '{}'.format(classes[int(box[0])])\n",
    "      current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "      #current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "  # Draw the predicted boxes in blue\n",
    "  for box in y_pred_decoded[i]:\n",
    "      xmin = box[-4]\n",
    "      ymin = box[-3]\n",
    "      xmax = box[-2]\n",
    "      ymax = box[-1]\n",
    "      color = colors[int(box[0])]\n",
    "      label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "      current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "      current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 480, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 480, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 480, 3)  0           identity_layer[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_stddev_normalization (Lam (None, 300, 480, 3)  0           input_mean_normalization[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 300, 480, 32) 2432        input_stddev_normalization[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 300, 480, 32) 128         conv1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu1 (ELU)                      (None, 300, 480, 32) 0           bn1[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 240, 32) 0           elu1[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 150, 240, 48) 13872       pool1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2 (BatchNormalization)        (None, 150, 240, 48) 192         conv2[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu2 (ELU)                      (None, 150, 240, 48) 0           bn2[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 120, 48)  0           elu2[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 75, 120, 64)  27712       pool2[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)        (None, 75, 120, 64)  256         conv3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu3 (ELU)                      (None, 75, 120, 64)  0           bn3[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 37, 60, 64)   0           elu3[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 37, 60, 64)   36928       pool3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn4 (BatchNormalization)        (None, 37, 60, 64)   256         conv4[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu4 (ELU)                      (None, 37, 60, 64)   0           bn4[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 18, 30, 64)   0           elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 18, 30, 48)   27696       pool4[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn5 (BatchNormalization)        (None, 18, 30, 48)   192         conv5[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu5 (ELU)                      (None, 18, 30, 48)   0           bn5[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 9, 15, 48)    0           elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 9, 15, 48)    20784       pool5[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn6 (BatchNormalization)        (None, 9, 15, 48)    192         conv6[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu6 (ELU)                      (None, 9, 15, 48)    0           bn6[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool6 (MaxPooling2D)            (None, 4, 7, 48)     0           elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 4, 7, 32)     13856       pool6[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn7 (BatchNormalization)        (None, 4, 7, 32)     128         conv7[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu7 (ELU)                      (None, 4, 7, 32)     0           bn7[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "classes4 (Conv2D)               (None, 37, 60, 24)   13848       elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes5 (Conv2D)               (None, 18, 30, 24)   10392       elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes6 (Conv2D)               (None, 9, 15, 24)    10392       elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes7 (Conv2D)               (None, 4, 7, 24)     6936        elu7[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes4 (Conv2D)                 (None, 37, 60, 16)   9232        elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes5 (Conv2D)                 (None, 18, 30, 16)   6928        elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes6 (Conv2D)                 (None, 9, 15, 16)    6928        elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes7 (Conv2D)                 (None, 4, 7, 16)     4624        elu7[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate (QuantizeAnno (None, None, 6)      0           classes4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_1 (QuantizeAn (None, None, 6)      0           classes5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_2 (QuantizeAn (None, None, 6)      0           classes6[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_3 (QuantizeAn (None, None, 6)      0           classes7[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors4 (AnchorBoxes)          (None, 37, 60, 4, 8) 0           boxes4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors5 (AnchorBoxes)          (None, 18, 30, 4, 8) 0           boxes5[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors6 (AnchorBoxes)          (None, 9, 15, 4, 8)  0           boxes6[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors7 (AnchorBoxes)          (None, 4, 7, 4, 8)   0           boxes7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "classes_concat (Concatenate)    (None, None, 6)      0           quantize_annotate[0][0]          \n",
      "                                                                 quantize_annotate_1[0][0]        \n",
      "                                                                 quantize_annotate_2[0][0]        \n",
      "                                                                 quantize_annotate_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_8 (QuantizeAn (None, None, 4)      0           boxes4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_9 (QuantizeAn (None, None, 4)      0           boxes5[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_10 (QuantizeA (None, None, 4)      0           boxes6[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_11 (QuantizeA (None, None, 4)      0           boxes7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_4 (QuantizeAn (None, None, 8)      0           anchors4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_5 (QuantizeAn (None, None, 8)      0           anchors5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_6 (QuantizeAn (None, None, 8)      0           anchors6[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quantize_annotate_7 (QuantizeAn (None, None, 8)      0           anchors7[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes_softmax (Activation)    (None, None, 6)      0           classes_concat[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "boxes_concat (Concatenate)      (None, None, 4)      0           quantize_annotate_8[0][0]        \n",
      "                                                                 quantize_annotate_9[0][0]        \n",
      "                                                                 quantize_annotate_10[0][0]       \n",
      "                                                                 quantize_annotate_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "anchors_concat (Concatenate)    (None, None, 8)      0           quantize_annotate_4[0][0]        \n",
      "                                                                 quantize_annotate_5[0][0]        \n",
      "                                                                 quantize_annotate_6[0][0]        \n",
      "                                                                 quantize_annotate_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 18)     0           classes_softmax[1][0]            \n",
      "                                                                 boxes_concat[1][0]               \n",
      "                                                                 anchors_concat[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 213,904\n",
      "Trainable params: 213,232\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "def apply_quantization_to_Conv2D(layer):\n",
    "  #print(layer)\n",
    "  if isinstance(layer, tf.keras.layers.Reshape):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "# to the layers of the model.\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization_to_Conv2D,\n",
    ")\n",
    "\n",
    "#annotated_model.save('quantize_ready_model_20_01_Conv2D.h5', include_optimizer=True)\n",
    "annotated_model.summary()\n",
    "# Now that the Dense layers are annotated,\n",
    "# `quantize_apply` actually makes the model quantization aware.\n",
    "\n",
    "with quantize_scope(\n",
    "  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "   'AnchorBoxes': AnchorBoxes}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "#quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "#quant_aware_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "#quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 480, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 480, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 480, 3)  0           identity_layer[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_stddev_normalization (Lam (None, 300, 480, 3)  0           input_mean_normalization[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 300, 480, 32) 2432        input_stddev_normalization[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 300, 480, 32) 128         conv1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu1 (ELU)                      (None, 300, 480, 32) 0           bn1[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 240, 32) 0           elu1[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 150, 240, 48) 13872       pool1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2 (BatchNormalization)        (None, 150, 240, 48) 192         conv2[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu2 (ELU)                      (None, 150, 240, 48) 0           bn2[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 120, 48)  0           elu2[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 75, 120, 64)  27712       pool2[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)        (None, 75, 120, 64)  256         conv3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu3 (ELU)                      (None, 75, 120, 64)  0           bn3[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 37, 60, 64)   0           elu3[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 37, 60, 64)   36928       pool3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn4 (BatchNormalization)        (None, 37, 60, 64)   256         conv4[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu4 (ELU)                      (None, 37, 60, 64)   0           bn4[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 18, 30, 64)   0           elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 18, 30, 48)   27696       pool4[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn5 (BatchNormalization)        (None, 18, 30, 48)   192         conv5[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu5 (ELU)                      (None, 18, 30, 48)   0           bn5[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 9, 15, 48)    0           elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 9, 15, 48)    20784       pool5[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn6 (BatchNormalization)        (None, 9, 15, 48)    192         conv6[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu6 (ELU)                      (None, 9, 15, 48)    0           bn6[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pool6 (MaxPooling2D)            (None, 4, 7, 48)     0           elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 4, 7, 32)     13856       pool6[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn7 (BatchNormalization)        (None, 4, 7, 32)     128         conv7[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu7 (ELU)                      (None, 4, 7, 32)     0           bn7[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "classes4 (Conv2D)               (None, 37, 60, 24)   13848       elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes5 (Conv2D)               (None, 18, 30, 24)   10392       elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes6 (Conv2D)               (None, 9, 15, 24)    10392       elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "classes7 (Conv2D)               (None, 4, 7, 24)     6936        elu7[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes4 (Conv2D)                 (None, 37, 60, 16)   9232        elu4[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes5 (Conv2D)                 (None, 18, 30, 16)   6928        elu5[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes6 (Conv2D)                 (None, 9, 15, 16)    6928        elu6[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "boxes7 (Conv2D)                 (None, 4, 7, 16)     4624        elu7[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "quant_classes4_reshape (Quantiz (None, None, 6)      1           classes4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_classes5_reshape (Quantiz (None, None, 6)      1           classes5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_classes6_reshape (Quantiz (None, None, 6)      1           classes6[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_classes7_reshape (Quantiz (None, None, 6)      1           classes7[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "anchors4 (AnchorBoxes)          (None, 37, 60, 4, 8) 0           boxes4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors5 (AnchorBoxes)          (None, 18, 30, 4, 8) 0           boxes5[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors6 (AnchorBoxes)          (None, 9, 15, 4, 8)  0           boxes6[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "anchors7 (AnchorBoxes)          (None, 4, 7, 4, 8)   0           boxes7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "classes_concat (Concatenate)    (None, None, 6)      0           quant_classes4_reshape[0][0]     \n",
      "                                                                 quant_classes5_reshape[0][0]     \n",
      "                                                                 quant_classes6_reshape[0][0]     \n",
      "                                                                 quant_classes7_reshape[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "quant_boxes4_reshape (QuantizeW (None, None, 4)      1           boxes4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quant_boxes5_reshape (QuantizeW (None, None, 4)      1           boxes5[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quant_boxes6_reshape (QuantizeW (None, None, 4)      1           boxes6[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quant_boxes7_reshape (QuantizeW (None, None, 4)      1           boxes7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "quant_anchors4_reshape (Quantiz (None, None, 8)      1           anchors4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_anchors5_reshape (Quantiz (None, None, 8)      1           anchors5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_anchors6_reshape (Quantiz (None, None, 8)      1           anchors6[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "quant_anchors7_reshape (Quantiz (None, None, 8)      1           anchors7[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classes_softmax (Activation)    (None, None, 6)      0           classes_concat[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "boxes_concat (Concatenate)      (None, None, 4)      0           quant_boxes4_reshape[0][0]       \n",
      "                                                                 quant_boxes5_reshape[0][0]       \n",
      "                                                                 quant_boxes6_reshape[0][0]       \n",
      "                                                                 quant_boxes7_reshape[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "anchors_concat (Concatenate)    (None, None, 8)      0           quant_anchors4_reshape[0][0]     \n",
      "                                                                 quant_anchors5_reshape[0][0]     \n",
      "                                                                 quant_anchors6_reshape[0][0]     \n",
      "                                                                 quant_anchors7_reshape[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 18)     0           classes_softmax[1][0]            \n",
      "                                                                 boxes_concat[1][0]               \n",
      "                                                                 anchors_concat[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 213,916\n",
      "Trainable params: 213,232\n",
      "Non-trainable params: 684\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_for_pruning.layers):\n",
    "    #print(model_for_pruning.layers)\n",
    "    if(layer._name == 'prune_low_magnitude_boxes4'):\n",
    "        layer._name='boxes4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
